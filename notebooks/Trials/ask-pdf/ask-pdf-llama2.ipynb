{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Application: Ask Questions from a PDF Document using Large Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval-Augmented Generation (RAG) is a generative AI framework that combines pre-trained large language models (LLMs) with external data sources. RAG improves the output of LLMs by using fresh data from authoritative knowledge bases and enterprise systems to generate more reliable responses.\n",
    "\n",
    "For example, this project is about using RAG to ask questions from a PDF document. The RAG system uses its large language model to understand the question, then it retrieves relevant information from the PDF document, and finally generates a response. This way, we can extract precise information from a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup Ollama\n",
    "\n",
    "I used [Ollama](https://ollama.com) because it's the easiest way to get up and running with large language models, locally on my computer.\n",
    "\n",
    "In this case, I used [Llama2](https://llama.meta.com/llama2) model by Meta AI as my choice.\n",
    "\n",
    "On your terminal, run:\n",
    "\n",
    "```bash\n",
    "ollama run llama2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Environment Variables and Setting Up the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# If you want to use the OpenAI API, you need to set the OPENAI_API_KEY environment variable\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "#MODEL = \"gpt-3.5-turbo\"\n",
    "\n",
    "MODEL = \"llama2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Embeddings and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "OllamaEndpointNotFoundError",
     "evalue": "Ollama call failed with status code 404. Maybe your model is not found and you should pull the model with `ollama pull llama2`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOllamaEndpointNotFoundError\u001b[0m               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m     model \u001b[38;5;241m=\u001b[39m Ollama(model\u001b[38;5;241m=\u001b[39mMODEL)\n\u001b[0;32m     11\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m OllamaEmbeddings(model\u001b[38;5;241m=\u001b[39mMODEL)\n\u001b[1;32m---> 13\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwhat is machine learning in a few words?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Desktop\\univalle\\tesis\\Tesis\\notebooks\\Trials\\ask-pdf\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:385\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    382\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    383\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 385\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    396\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    397\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Desktop\\univalle\\tesis\\Tesis\\notebooks\\Trials\\ask-pdf\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:750\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    744\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    748\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    749\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Desktop\\univalle\\tesis\\Tesis\\notebooks\\Trials\\ask-pdf\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:944\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    930\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    931\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    932\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    942\u001b[0m         )\n\u001b[0;32m    943\u001b[0m     ]\n\u001b[1;32m--> 944\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Desktop\\univalle\\tesis\\Tesis\\notebooks\\Trials\\ask-pdf\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:787\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[0;32m    786\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 787\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    788\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Desktop\\univalle\\tesis\\Tesis\\notebooks\\Trials\\ask-pdf\\.conda\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:774\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    766\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    771\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 774\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    777\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[0;32m    778\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    781\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    782\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    783\u001b[0m         )\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    785\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Desktop\\univalle\\tesis\\Tesis\\notebooks\\Trials\\ask-pdf\\.conda\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:429\u001b[0m, in \u001b[0;36mOllama._generate\u001b[1;34m(self, prompts, stop, images, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    427\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    428\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m--> 429\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    437\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([final_chunk])\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Desktop\\univalle\\tesis\\Tesis\\notebooks\\Trials\\ask-pdf\\.conda\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:347\u001b[0m, in \u001b[0;36m_OllamaCommon._stream_with_aggregation\u001b[1;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream_with_aggregation\u001b[39m(\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    340\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    345\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GenerationChunk:\n\u001b[0;32m    346\u001b[0m     final_chunk: Optional[GenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_generate_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m_stream_response_to_generation_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstream_resp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Desktop\\univalle\\tesis\\Tesis\\notebooks\\Trials\\ask-pdf\\.conda\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:192\u001b[0m, in \u001b[0;36m_OllamaCommon._create_generate_stream\u001b[1;34m(self, prompt, stop, images, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_generate_stream\u001b[39m(\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    186\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    190\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    191\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: images}\n\u001b[1;32m--> 192\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_stream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpayload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/api/generate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Usuario\\Desktop\\univalle\\tesis\\Tesis\\notebooks\\Trials\\ask-pdf\\.conda\\Lib\\site-packages\\langchain_community\\llms\\ollama.py:264\u001b[0m, in \u001b[0;36m_OllamaCommon._create_stream\u001b[1;34m(self, api_url, payload, stop, **kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m    263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n\u001b[1;32m--> 264\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m OllamaEndpointNotFoundError(\n\u001b[0;32m    265\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOllama call failed with status code 404. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    266\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaybe your model is not found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    267\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand you should pull the model with `ollama pull \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    268\u001b[0m         )\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    270\u001b[0m         optional_detail \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mtext\n",
      "\u001b[1;31mOllamaEndpointNotFoundError\u001b[0m: Ollama call failed with status code 404. Maybe your model is not found and you should pull the model with `ollama pull llama2`."
     ]
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "if MODEL.startswith(\"gpt\"):\n",
    "    model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=MODEL)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "else:\n",
    "    model = Ollama(model=MODEL)\n",
    "    embeddings = OllamaEmbeddings(model=MODEL)\n",
    "\n",
    "model.invoke(\"what is machine learning in a few words?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMachine learning is a subfield of artificial intelligence that involves training algorithms to learn from data and make predictions or decisions without being explicitly programmed.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = model | parser \n",
    "chain.invoke(\"what is machine learning in a few words?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load the PDF Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"3/10/24, 4:22 PM OpenAI and Elon Musk\\nhttps://openai.com/blog/openai-elon-musk 1/13OpenAI and Elon Musk\\nWe are dedicated to the OpenAI mission and have pursued it every step of the\\nway.\\nMarch 5, 2024\\nAuthorsBlog\\nGreg Brockman\\nIlya Sutskever\\nJohn Schulman\\nSam Altman\\nWojciech Zaremba\\nOpenAI\\nAnnouncements\\nThe mission of OpenAI is to ensure AGI benefits all of humanity, which means both building safe and\\nbeneficial AGI and helping create broadly distributed benefits. We are now sharing what we've\\nlearned about achieving our mission, and some facts about our relationship with Elon. We intend to\\nmove to dismiss all of Elon’s claims.\", metadata={'source': 'openai-and-elon.pdf', 'page': 0}),\n",
       " Document(page_content=\"3/10/24, 4:22 PM OpenAI and Elon Musk\\nhttps://openai.com/blog/openai-elon-musk 2/13We realized building AGI will require far more resources than we’d initially\\nimagined\\nElon said we should announce an initial $1B funding commitment to OpenAI. In total, the non-profit\\nhas raised less than $45M from Elon and more than $90M from other donors.\\nWhen starting OpenAI in late 2015, Greg and Sam had initially planned to raise $100M. Elon said in an\\nemail: “We need to go with a much bigger number than $100M to avoid sounding hopeless… I think\\nwe should say that we are starting with a $1B funding commitment… I will cover whatever anyone\\nelse doesn't provide.” \\nWe spent a lot of time trying to envision a plausible path to AGI. In early 2017, we came to the\\nrealization that building AGI will require vast quantities of compute. We began calculating how much\\ncompute an AGI might plausibly require. We all understood we were going to need a lot more capital\\nto succeed at our mission—billions of dollars per year, which was far more than any of us, especially\\nElon, thought we’d be able to raise as the non-profit.[1]\\nWe and Elon recognized a for-profit entity would be necessary to acquire\\nthose resources\\nAs we discussed a for-profit structure in order to further the mission, Elon wanted us to merge with\\nTesla or he wanted full control. Elon left OpenAI, saying there needed to be a relevant competitor to\\nGoogle/DeepMind and that he was going to do it himself. He said he’d be supportive of us finding our\\nown path.\\nIn late 2017, we and Elon decided the next step for the mission was to create a for-profit entity. Elon\\nwanted majority equity, initial board control, and to be CEO. In the middle of these discussions, he\\nwithheld funding. Reid Hoffman bridged the gap to cover salaries and operations.\\nWe couldn’t agree to terms on a for-profit with Elon because we felt it was against the mission for any\\nindividual to have absolute control over OpenAI. He then suggested instead merging OpenAI into\\nTesla. In early February 2018, Elon forwarded us an email suggesting that OpenAI should “attach to\\nTesla as its cash cow”, commenting that it was “exactly right… Tesla is the only path that could even\\nhope to hold a candle to Google. Even then, the probability of being a counterweight to Google is\\nsmall. It just isn’t zero”. \\nElon soon chose to leave OpenAI, saying that our probability of success was 0, and that he planned\\nto build an AGI competitor within Tesla. When he left in late February 2018, he told our team he was\\nsupportive of us finding our own path to raising billions of dollars. In December 2018, Elon sent us an\\nemail saying “Even raising several hundred million won’t be enough. This needs billions per year\\nimmediately or forget it.” [2]\\n[3]\", metadata={'source': 'openai-and-elon.pdf', 'page': 1}),\n",
       " Document(page_content=\"3/10/24, 4:22 PM OpenAI and Elon Musk\\nhttps://openai.com/blog/openai-elon-musk 3/13We advance our mission by building widely-available beneficial tools\\nWe’re making our technology broadly usable in ways that empower people and improve their daily\\nlives, including via open-source contributions.\\nWe provide broad access to today's most powerful AI, including a free version that hundreds of\\nmillions of people use every day. For example, Albania is using OpenAI’s tools to accelerate its EU\\naccession by as much as 5.5 years; Digital Green is helping boost farmer income in Kenya and India\\nby dropping the cost of agricultural extension services 100x by building on OpenAI; Lifespan, the\\nlargest healthcare provider in Rhode Island, uses GPT-4 to simplify its surgical consent forms from a\\ncollege reading level to a 6th grade one; Iceland is using GPT-4 to preserve the Icelandic language.\\nElon understood the mission did not imply open-sourcing AGI. As Ilya told Elon: “As we get closer to\\nbuilding AI, it will make sense to start being less open.\\xa0 The Open in openAI means that everyone\\nshould benefit from the fruits of AI after its built, but it's totally OK to not share the science...”, to\\nwhich Elon replied: “Yup”. [4]\\nWe're sad that it's come to this with someone whom we’ve deeply admired—someone who inspired\\nus to aim higher, then told us we would fail, started a competitor, and then sued us when we started\\nmaking meaningful progress towards OpenAI’s mission without him.\\nWe are focused on advancing our mission and have a long way to go. As we continue to make our\\ntools better and better, we are excited to deploy these systems so they empower every individual.\\n[1]\\nFrom:\\xa0 Elon Musk < >\\nTo:\\xa0 Greg Brockman < >\\nCC:\\xa0 Sam Altman < >\\nDate: Sun, Nov 22, 2015 at 7\\x0048 PM\\nSubject: follow up from call\\nBlog sounds good, assuming adjustments for neutrality vs being YC-centric.\\nI'd favor positioning the blog to appeal a bit more to the general public -- there is a lot of value to\\nhaving the public root for us to succeed -- and then having a longer, more detailed and inside-\\nbaseball version for recruiting, with a link to it at the end of the general public version.\\nWe need to go with a much bigger number than $100M to avoid sounding hopeless relative to what\\nGoogle or Facebook are spending. I think we should say that we are starting with a $1B funding\\ncommitment. This is real. I will cover whatever anyone else doesn't provide.\", metadata={'source': 'openai-and-elon.pdf', 'page': 2}),\n",
       " Document(page_content=\"3/10/24, 4:22 PM OpenAI and Elon Musk\\nhttps://openai.com/blog/openai-elon-musk 4/13Template seems fine, apart from shifting to a vesting cash bonus as default, which can optionally be\\nturned into YC or potentially SpaceX (need to understand how much this will be) stock.\\n[2]\\nFrom:\\xa0 Elon Musk < >\\nTo:\\xa0 Ilya Sutskever < >, Greg Brockman < >\\nDate: Thu, Feb 1, 2018 at 3\\x0052 AM\\nSubject: Fwd: Top AI institutions today\\n is exactly right. We may wish it otherwise, but, in my and ’s opinion, Tesla is the only\\npath that could even hope to hold a candle to Google. Even then, the probability of being a\\ncounterweight to Google is small. It just isn't zero.\\nBegin forwarded message:\\nFrom:\\xa0  < >\\nTo:\\xa0 Elon Musk < >\\nDate: January 31, 2018 at 11\\x0054\\x0030 PM PST\\nSubject: Re: Top AI institutions today\\nWorking at the cutting edge of AI is unfortunately expensive. For example,\\nIn\\naddition to DeepMind, Google also has Google Brain, Research, and Cloud. And TensorFlow,\\nTPUs, and they own about a third of all research (in fact, they hold their own AI conferences).\\nI also strongly suspect that compute horsepower will be necessary (and possibly even sufficient)\\nto reach AGI. If historical trends are any indication, progress in AI is primarily driven by systems -\\ncompute, data, infrastructure. The core algorithms we use today have remained largely\\nunchanged from the ~90s. Not only that, but any algorithmic advances published in a paper\\nsomewhere can be almost immediately re-implemented and incorporated. Conversely,\\nalgorithmic advances alone are inert without the scale to also make them scary.\\nIt seems to me that OpenAI today is burning cash and that the funding model cannot reach the\\nscale to seriously compete with Google (an 800B company). If you can't seriously compete but\\ncontinue to do research in open, you might in fact be making things worse and helping them out\\n“for free”, because any advances are fairly easy for them to copy and immediately incorporate, at\\nscale.\\nA for-profit pivot might create a more sustainable revenue stream over time and would, with the\\ncurrent team, likely bring in a lot of investment. However, building out a product from scratch\\nwould steal focus from AI research, it would take a long time and it's unclear if a company could\\n“catch up” to Google scale, and the investors might exert too much pressure in the wrong\\ndirections.The most promising option I can think of, as I mentioned earlier, would be for OpenAI to\", metadata={'source': 'openai-and-elon.pdf', 'page': 3}),\n",
       " Document(page_content=\"3/10/24, 4:22 PM OpenAI and Elon Musk\\nhttps://openai.com/blog/openai-elon-musk 5/13attach to Tesla as its cash cow. I believe attachments to other large suspects (e.g. Apple?\\nAmazon?) would fail due to an incompatible company DNA. Using a rocket analogy, Tesla already\\nbuilt the “first stage” of the rocket with the whole supply chain of Model 3 and its onboard\\ncomputer and a persistent internet connection. The “second stage” would be a full self driving\\nsolution based on large-scale neural network training, which OpenAI expertise could significantly\\nhelp accelerate. With a functioning full self-driving solution in ~2-3 years we could sell a lot of\\ncars/trucks. If we do this really well, the transportation industry is large enough that we could\\nincrease Tesla's market cap to high O(~100K), and use that revenue to fund the AI work at the\\nappropriate scale.\\nI cannot see anything else that has the potential to reach sustainable Google-scale capital within a\\ndecade.\\n[3]\\nFrom:\\xa0 Elon Musk < >\\nTo:\\xa0 Ilya Sutskever < >, Greg Brockman < >\\nCC:\\xa0 Sam Altman < >,  < >\\nDate: Wed, Dec 26, 2018 at 12\\x0007 PM\\nSubject: I feel I should reiterate\\nMy probability assessment of OpenAI being relevant to DeepMind/Google without a dramatic\\nchange in execution and resources is 0%. Not 1%. I wish it were otherwise.\\nEven raising several hundred million won't be enough. This needs billions per year immediately or\\nforget it.\\nUnfortunately, humanity's future is in the hands of .\\nAnd they are doing a lot more than this.\\nI really hope I'm wrong.\\nElon\\n[4]\", metadata={'source': 'openai-and-elon.pdf', 'page': 4}),\n",
       " Document(page_content=\"3/10/24, 4:22 PM OpenAI and Elon Musk\\nhttps://openai.com/blog/openai-elon-musk 6/13Fwd: congrats on the falcon 9\\n3 messages\\nFrom:\\xa0 Elon Musk < >\\nTo:\\xa0 Sam Altman < >, Ilya Sutskever < >, Greg Brockman <\\n>\\nDate: Sat, Jan 2, 2016 at 8\\x0018 AM\\nSubject: Fwd: congrats on the falcon 9\\nBegin forwarded message:\\nFrom:\\xa0  < >\\nTo:\\xa0 Elon Musk < >\\nDate: January 2, 2016 at 10\\x0012\\x0032 AM CST\\nSubject: congrats on the falcon 9\\nHi Elon\\nHappy new year to you, !\\nCongratulations on landing the Falcon 9, what an amazing achievement. Time to build out the\\nfleet now!\\nI've seen you (and Sam and other OpenAI people) doing a lot of interviews recently extolling the\\nvirtues of open sourcing AI, but I presume you realise that this is not some sort of panacea that\\nwill somehow magically solve the safety problem? There are many good arguments as to why the\\napproach you are taking is actually very dangerous and in fact may increase the risk to the world.\\nSome of the more obvious points are well articulated in this blog post, that I'm sure you've seen,\\nbut there are also other important considerations:\\nhttp://slatestarcodex.com/2015/12/17/should-ai-be-open/\\nI’d be interested to hear your counter-arguments to these points.\\nBest\\nFrom:\\xa0 Ilya Sutskever < >\\nTo:\\xa0 Elon Musk < >, Sam Altman < >, Greg Brockman <\\n>\\nDate: Sat, Jan 2, 2016 at 9\\x0006 AM\\nSubject: Fwd: congrats on the falcon 9\\nThe article is concerned with a hard takeoff scenario: if a hard takeoff occurs, and a safe AI is harder\\nto build than an unsafe one, then by opensorucing everything, we make it easy for someone\\nunscrupulous with access to overwhelming amount of hardware to build an unsafe AI, which will\\nexperience a hard takeoff.\", metadata={'source': 'openai-and-elon.pdf', 'page': 5}),\n",
       " Document(page_content=\"3/10/24, 4:22 PM OpenAI and Elon Musk\\nhttps://openai.com/blog/openai-elon-musk 7/13Authors\\nRelated researchGreg Brockman\\nIlya Sutskever\\nJohn Schulman\\nSam Altman\\nWojciech Zaremba\\nOpenAIView all articles\\nView all articles\\nView all articles\\nView all articles\\nView all articles\\nView all articles\\nView all researchAs we get closer to building AI, it will make sense to start being less open. The Open in openAI\\nmeans that everyone should benefit from the fruits of AI after its built, but it's totally OK to not share\\nthe science (even though sharing everything is definitely the right strategy in the short and\\npossibly medium term for recruitment purposes).\\nFrom:\\xa0 Elon Musk < >\\nTo:\\xa0 Ilya Sutskever < >\\nDate: Sat, Jan 2, 2016 at 9\\x0011 AM\\nSubject: Fwd: congrats on the falcon 9\\nYup\", metadata={'source': 'openai-and-elon.pdf', 'page': 6})]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"openai-and-elon.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer the question based on the context below. If you can't \n",
      "answer the question, reply \"I don't know\".\n",
      "\n",
      "Context: Here is some context\n",
      "\n",
      "Question: Here is a question\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "input_context = input(\"Enter the context: \")\n",
    "input_question = input(\"Enter the question: \")\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "print(prompt.format(context=input_context, question=input_question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Chain the Prompt, Model, and Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'PromptInput',\n",
       " 'type': 'object',\n",
       " 'properties': {'context': {'title': 'Context', 'type': 'string'},\n",
       "  'question': {'title': 'Question', 'type': 'string'}}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.input_schema.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Based on the context you provided, I would estimate that your college background is likely in a field related to computer science or engineering, such as computer engineering, electrical engineering, or data science. Your specialty in machine learning suggests that you have a strong foundation in programming and algorithms, as well as statistical analysis and data modeling.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"context\": \"my specialty is machine learning\", \n",
    "        \"question\": \"what do you think is my college background?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Use a Vector Database to Store and Retrieve the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\xl38\\Desktop\\ml-projects\\ml\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_documents(pages, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is OpenAI's mission?\n",
      "Answer: Based on the provided documents, OpenAI's mission is to ensure that artificial general intelligence (AGI) benefits all of humanity. They aim to build safe and beneficial AGI and help create broadly distributed benefits. OpenAI also wants to move towards dismissing Elon Musk's claims about their relationship with him.\n",
      "\n",
      "Question: What did they realized?\n",
      "Answer: From the emails, it seems that Elon Musk and OpenAI realized several things:\n",
      "\n",
      "1. The potential danger of open-sourcing AI: Ilya Sutskever expressed concerns in an email to Elon Musk about the potential risks of open-sourcing AI, including the possibility of someone unscrupulous with access to overwhelming amount of hardware building an unsafe AI.\n",
      "2. The importance of scaling AI research: Elon Musk expressed his belief that OpenAI had the potential to reach sustainable Google-scale capital within a decade if they focused on scaling their AI research.\n",
      "3. The need for significant resources: Both Elon Musk and Ilya Sutskever emphasized the need for significant resources, including billions of dollars per year, in order to execute their plans for OpenAI.\n",
      "4. The potential for Tesla to be a cash cow: Elon Musk mentioned that he believed attachments to other large suspects (e.g. Apple? Amazon?) would fail due to an incompatible company DNA, and that Tesla already built the “first stage” of the rocket with the whole supply chain of Model 3 and its onboard computer and a persistent internet connection, making it a potential cash cow for OpenAI.\n",
      "5. The importance of collaboration: Both Elon Musk and Ilya Sutskever emphasized the importance of collaboration between OpenAI and other companies, including Tesla, in order to accelerate the development of AI.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What is OpenAI's mission?\",\n",
    "    \"What did they realized?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {chain.invoke({'question': question})}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Streaming Questions to the Language Model\n",
    "Basically, what stream does is make the response appear like the style of a chatbot because of a typewriter effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, the answer to the question is: Greg Brockman, Ilya Sutskever, John Schulman, Sam Altman, and Wojciech Zaremba."
     ]
    }
   ],
   "source": [
    "for s in chain.stream({\"question\": \"Who are the authors of the blog?\"}):\n",
    "    print(s, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Batching Questions to the Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, what batch does is that it allows you to send a batch of questions to the model. This is useful when you have a lot of questions to ask and you don't want to wait for the model to process each question one by one. This is done in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"tell me the challenges that OpenAI faced\",\n",
    "    #\"another question here\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Based on the email conversation between Elon Musk and Greg Brockman, OpenAI faced several challenges:\\n\\n1. Financial constraints: OpenAI faced financial difficulties in competing with larger companies like Google, which had a much larger budget for AI research.\\n2. Lack of scalability: OpenAI\\'s compute horsepower was not sufficient to reach AGI, and it was unclear if the organization could \"catch up\" to Google scale.\\n3. Pressure from investors: OpenAI\\'s funding model was not sustainable, and investors might exert too much pressure in the wrong directions.\\n4. Competition from Elon Musk: Elon Musk started a competitor to OpenAI, which added an extra layer of competition for the organization.\\n5. Balancing open-source contributions with proprietary technology: OpenAI had to balance its mission of making AI beneficial tools widely available with the need to protect its proprietary technology.\\n6. Neutrality vs YC-centricity: OpenAI had to navigate the challenge of positioning itself as a neutral organization while also appealing to the general public and recruiting top talent.\\n7. Size of funding commitment: OpenAI had to go with a much bigger number than $100M to avoid sounding hopeless relative to what Google or Facebook are spending.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "chain.batch([{\"question\": q} for q in questions])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
