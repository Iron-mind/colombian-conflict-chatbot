{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generar instancias para entrenar el modelo Colombian-conflict-chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-1.68.2-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting PyMuPDF\n",
      "  Downloading pymupdf-1.25.4-cp39-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting python-dotenv\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting numpy>=1.23.2 (from pandas)\n",
      "  Downloading numpy-2.2.4-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\usuario\\desktop\\univalle\\tesis\\tesis\\notebooks\\utils\\.conda\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.9.0-cp311-cp311-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\usuario\\desktop\\univalle\\tesis\\tesis\\notebooks\\utils\\.conda\\lib\\site-packages (from openai) (4.12.2)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.27.2-cp311-cp311-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\usuario\\desktop\\univalle\\tesis\\tesis\\notebooks\\utils\\.conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\usuario\\desktop\\univalle\\tesis\\tesis\\notebooks\\utils\\.conda\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Using cached pandas-2.2.3-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "Downloading openai-1.68.2-py3-none-any.whl (606 kB)\n",
      "   ---------------------------------------- 0.0/606.1 kB ? eta -:--:--\n",
      "   --------------------------------------- 606.1/606.1 kB 10.7 MB/s eta 0:00:00\n",
      "Downloading pymupdf-1.25.4-cp39-abi3-win_amd64.whl (16.6 MB)\n",
      "   ---------------------------------------- 0.0/16.6 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 2.4/16.6 MB 12.2 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 4.7/16.6 MB 11.9 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 7.1/16.6 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 9.4/16.6 MB 12.0 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 11.8/16.6 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 14.4/16.6 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.6/16.6 MB 11.7 MB/s eta 0:00:00\n",
      "Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "Downloading jiter-0.9.0-cp311-cp311-win_amd64.whl (210 kB)\n",
      "Downloading numpy-2.2.4-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 2.1/12.9 MB 9.8 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 4.5/12.9 MB 11.2 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 7.1/12.9 MB 11.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 9.4/12.9 MB 11.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.1/12.9 MB 11.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.9/12.9 MB 11.4 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 12.3 MB/s eta 0:00:00\n",
      "Using cached pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Installing collected packages: pytz, tzdata, tqdm, sniffio, python-dotenv, PyMuPDF, pydantic-core, numpy, jiter, idna, h11, distro, certifi, annotated-types, pydantic, pandas, httpcore, anyio, httpx, openai\n",
      "Successfully installed PyMuPDF-1.25.4 annotated-types-0.7.0 anyio-4.9.0 certifi-2025.1.31 distro-1.9.0 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 idna-3.10 jiter-0.9.0 numpy-2.2.4 openai-1.68.2 pandas-2.2.3 pydantic-2.10.6 pydantic-core-2.27.2 python-dotenv-1.0.1 pytz-2025.1 sniffio-1.3.1 tqdm-4.67.1 tzdata-2025.2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install pandas openai PyMuPDF python-dotenv openpyxl tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import fitz  # PyMuPDF for PDF text extraction\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from tiktoken import encoding_for_model\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reports_df = pd.read_excel(\"../../resources/listado-informes.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text(file_path):\n",
    "    if file_path.lower().endswith('.pdf'):\n",
    "        text = \"\"\n",
    "        doc = fitz.open(file_path)\n",
    "        for page in doc:\n",
    "            text += page.get_text()\n",
    "        return text.strip()\n",
    "    elif file_path.lower().endswith('.csv'):\n",
    "        df = pd.read_csv(file_path)\n",
    "        return df.to_string(index=False).strip()\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_prompts(ident, title):\n",
    "    prompts = []\n",
    "    for i in range(1, 4):\n",
    "        prompt_path = f\"prompts/basic_question{i}.txt\"\n",
    "        with open(prompt_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            prompt = file.read().replace(\"{{report_name}}\", f\"{ident} {title}\")\n",
    "            prompts.append(prompt)\n",
    "    return prompts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=openai.api_key)\n",
    "model_name = \"gpt-4o\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Count tokens\n",
    "def count_tokens(text, model=model_name):\n",
    "    encoding = encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# Split text into chunks based on token limits\n",
    "def split_text_into_chunks(text, max_tokens, model=model_name):\n",
    "    words = text.split()\n",
    "    chunks, current_chunk = [], []\n",
    "\n",
    "    for word in words:\n",
    "        current_chunk.append(word)\n",
    "        if count_tokens(\" \".join(current_chunk), model) >= max_tokens:\n",
    "            chunks.append(\" \".join(current_chunk[:-1]))\n",
    "            current_chunk = [word]\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# Generate instances with OpenAI's modern API\n",
    "import re\n",
    "\n",
    "def extract_json_from_response(response_text):\n",
    "    try:\n",
    "        # Extraer contenido JSON dentro de ```json ```\n",
    "        json_match = re.search(r'```json\\s*(.*?)\\s*```', response_text, re.DOTALL)\n",
    "        if json_match:\n",
    "            json_content = json_match.group(1)\n",
    "        else:\n",
    "            json_content = response_text.strip()\n",
    "        return json.loads(json_content)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"JSON Decode Error: {e}\")\n",
    "        print(f\"Original response: {response_text}\")\n",
    "        return None\n",
    "\n",
    "def generate_instances(text_chunks, prompts):\n",
    "    combined_text = \"\\n\\n\".join(text_chunks)\n",
    "    full_prompt = f\"{combined_text}\\n\\n\" + \"\\n\\n\".join(prompts) + \"\\n\\n\" \\\n",
    "                  \"Please generate exactly three question-answer-context sets in JSON format. \" \\\n",
    "                  \"Return the JSON response directly, without explanations or additional text.\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You generate questions and answers from documents, returning JSON only.\"},\n",
    "                {\"role\": \"user\", \"content\": full_prompt}\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "            max_tokens=3000\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        print(f\"Generated content: {content}\")\n",
    "        instances = extract_json_from_response(content)\n",
    "        return instances\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating instances: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated content: ```json\n",
      "[\n",
      "    {\n",
      "        \"question\": \"¿Cuáles son los hallazgos principales del informe 058-CI-00233 Informe agroarte Colombia: Red Territorial de la Memoria sobre el conflicto armado colombiano?\",\n",
      "        \"context\": \"Los argumentos conforme a los que estimamos necesario realizar actuaciones efectivas destinadas al reconocimiento y la protección de los lugares memoriales generados por la interacción de víctimas y colectivos sociales en los territorios entroncan con dos elementos esenciales de la realidad colombiana actual: - El valor de estos espacios para la consecución de una paz estable y duradera por el papel que potencialmente les corresponde en la construcción de memorias e identidades locales, - La ausencia, en el ordenamiento jurídico colombiano, de fórmulas articuladas que sean útiles para la defensa de su pervivencia en el tiempo.\",\n",
      "        \"answer\": \"El informe destaca la importancia de los espacios de memoria para construir una paz duradera en Colombia, subrayando su papel en la creación de memorias e identidades locales. Además, identifica la falta de un marco jurídico adecuado para proteger estos lugares, lo que amenaza su continuidad y su capacidad para contribuir a la reconciliación y la no repetición del conflicto.\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"¿Qué dice este informe 058-CI-00233 Informe agroarte Colombia: Red Territorial de la Memoria sobre las causas estructurales del conflicto armado y sus recomendaciones para la no repetición?\",\n",
      "        \"context\": \"Si el objetivo es que la memoria histórica de la época en la que se desarrolló el conflicto armado en Colombia cumpla con las funciones de reparar a las víctimas, evitar la repetición y confrontar el olvido que se traduce en impunidad, es necesario reflexionar acerca de cuales son los mecanismos a través de los que estamos generando los relatos que van a conformar la memoria histórica y si se fomenta la participación de la sociedad en su configuración.\",\n",
      "        \"answer\": \"El informe sugiere que para abordar las causas estructurales del conflicto armado en Colombia, es fundamental fomentar la participación de la sociedad en la creación de relatos históricos. Recomienda que estos relatos sean inclusivos y reflejen las diversas experiencias de las víctimas, lo que contribuiría a la reparación, evitaría la repetición y confrontaría el olvido que lleva a la impunidad.\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"¿Qué iniciativas propone el informe 058-CI-00233 Informe agroarte Colombia: Red Territorial de la Memoria para proteger los lugares de memoria del conflicto armado?\",\n",
      "        \"context\": \"Reconocer la transcendencia que estos lugares tienen para la consecución de una paz estable y duradera y reivindicar el valor de las iniciativas que han logrado configurarlos nos lleva necesariamente a considerar indispensable un mecanismo adecuado de identificación, gestión y protección de los sitios de memoria generados por la interacción de victimas y acción civil.\",\n",
      "        \"answer\": \"El informe propone la creación de un marco jurídico específico que garantice la identificación, gestión y protección de los sitios de memoria. Estas medidas son necesarias para asegurar que los lugares de memoria, creados por la interacción de víctimas y colectivos sociales, puedan continuar desempeñando su papel en la construcción de una paz estable y duradera.\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "Waiting 60 seconds to respect token rate limits...\n",
      "Error generating instances: Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-Kk84fOnCscz4YWYFFemyp2rv on tokens per min (TPM): Limit 30000, Requested 49324. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Waiting 60 seconds to respect token rate limits...\n",
      "Error generating instances: Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-Kk84fOnCscz4YWYFFemyp2rv on tokens per min (TPM): Limit 30000, Requested 103807. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Waiting 60 seconds to respect token rate limits...\n",
      "Generated content: ```json\n",
      "[\n",
      "    {\n",
      "        \"question\": \"¿Cuáles son los hallazgos principales del informe 058-CI-00628 Suroeste antioqueño: un conflicto silenciado. Aproximación a la construcción de memoria histórica del conflicto armado en el suroeste antioqueño 1984-2016 sobre el conflicto armado colombiano?\",\n",
      "        \"context\": \"El informe detalla cómo el conflicto armado en el suroeste antioqueño estuvo marcado por la presencia de grupos armados ilegales que ejercieron control territorial y cometieron violaciones a los derechos humanos, incluyendo desplazamiento forzado, desapariciones forzadas y violencia sexual. Además, se resalta la falta de presencia estatal y la complicidad de algunos sectores con los actores armados.\",\n",
      "        \"answer\": \"El informe destaca que el conflicto armado en el suroeste antioqueño estuvo caracterizado por el control territorial de grupos armados ilegales, quienes perpetraron violaciones a los derechos humanos como desplazamiento forzado, desapariciones y violencia sexual. La ausencia del Estado y la complicidad de ciertos sectores con los actores armados exacerbaron la situación.\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"¿Qué eventos específicos documenta el informe 058-CI-00628 Suroeste antioqueño: un conflicto silenciado sobre el conflicto armado en Colombia?\",\n",
      "        \"context\": \"El informe documenta eventos específicos como masacres en comunidades rurales, el reclutamiento forzado de menores por parte de grupos armados ilegales y el uso de minas antipersonales que afectaron a la población civil. También se registran testimonios de víctimas que relatan el impacto de estos actos en sus vidas.\",\n",
      "        \"answer\": \"El informe documenta masacres en comunidades rurales, reclutamiento forzado de menores y el uso de minas antipersonales por grupos armados ilegales. Testimonios de víctimas destacan el impacto devastador de estos actos en sus vidas y comunidades.\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"¿Qué dice este informe 058-CI-00628 Suroeste antioqueño: un conflicto silenciado sobre las causas estructurales del conflicto armado y sus recomendaciones para la no repetición?\",\n",
      "        \"context\": \"El informe identifica causas estructurales del conflicto armado como la desigualdad socioeconómica, la falta de acceso a la tierra y la debilidad institucional. Recomienda fortalecer la presencia estatal en las zonas afectadas, implementar políticas de desarrollo rural y promover la reconciliación y el diálogo entre las comunidades.\",\n",
      "        \"answer\": \"El informe señala que la desigualdad socioeconómica, la falta de acceso a la tierra y la debilidad institucional son causas estructurales del conflicto armado. Recomienda fortalecer la presencia estatal, implementar políticas de desarrollo rural y fomentar la reconciliación para evitar la repetición de la violencia.\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "Waiting 60 seconds to respect token rate limits...\n",
      "Error generating instances: Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-Kk84fOnCscz4YWYFFemyp2rv on tokens per min (TPM): Limit 30000, Requested 74249. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Waiting 60 seconds to respect token rate limits...\n",
      "Error generating instances: Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-Kk84fOnCscz4YWYFFemyp2rv on tokens per min (TPM): Limit 30000, Requested 126722. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Waiting 60 seconds to respect token rate limits...\n",
      "Error generating instances: Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-Kk84fOnCscz4YWYFFemyp2rv on tokens per min (TPM): Limit 30000, Requested 70428. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Waiting 60 seconds to respect token rate limits...\n",
      "Generated content: ```json\n",
      "[\n",
      "    {\n",
      "        \"question\": \"¿Cuáles son los hallazgos principales del informe 058-CI-00772 Por la reivindicación del ser campesino sobre el conflicto armado colombiano?\",\n",
      "        \"context\": \"La Asociación de Campesinos de Ituango documentó hechos de violencia en su territorio, involucrando a actores armados como el Ejército, FARC, Paramilitares y ELN. Se identificaron 176 homicidios, 18 desapariciones forzadas, 542 desplazamientos forzados, entre otros. Los testimonios recogidos señalan la responsabilidad de múltiples actores armados en estos hechos.\",\n",
      "        \"answer\": \"El informe destaca la documentación de numerosos actos de violencia en Ituango, incluyendo homicidios, desapariciones forzadas y desplazamientos masivos, atribuidos a diversos actores armados como el Ejército, FARC, Paramilitares y ELN. Estos hallazgos subrayan la complejidad del conflicto armado en la región y la necesidad de reconstruir la memoria histórica para reconocer a las víctimas y evitar la repetición de estos hechos.\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"¿Qué eventos específicos de violencia documenta el informe 058-CI-00772 Por la reivindicación del ser campesino en Ituango?\",\n",
      "        \"context\": \"El informe documenta eventos como la masacre de El Aro en 1997 por paramilitares, el desplazamiento forzado y quema de casas en la vereda Leones en 2000 por paramilitares y ejército, y la toma armada de Santa Rita en 2000 por FARC y paramilitares. Estos eventos incluyen testimonios de víctimas que describen la violencia sufrida.\",\n",
      "        \"answer\": \"El informe documenta eventos específicos de violencia en Ituango, como la masacre de El Aro en 1997, donde paramilitares desplazaron y asesinaron a civiles, y la toma armada de Santa Rita en 2000 por FARC y paramilitares, que resultó en saqueos y tratos crueles. Los testimonios de las víctimas proporcionan un relato detallado de estos actos violentos.\"\n",
      "    },\n",
      "    {\n",
      "        \"question\": \"¿Qué dice este informe 058-CI-00772 Por la reivindicación del ser campesino sobre las causas estructurales del conflicto armado y sus recomendaciones para la no repetición?\",\n",
      "        \"context\": \"\",\n",
      "        \"answer\": \"\"\n",
      "    }\n",
      "]\n",
      "```\n",
      "Waiting 60 seconds to respect token rate limits...\n",
      "Error generating instances: Error code: 429 - {'error': {'message': 'Request too large for gpt-4o in organization org-Kk84fOnCscz4YWYFFemyp2rv on tokens per min (TPM): Limit 30000, Requested 75976. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}\n",
      "Waiting 60 seconds to respect token rate limits...\n"
     ]
    }
   ],
   "source": [
    "# Main loop to process reports and generate dataset instances\n",
    "reports_folder = \"../../reports-pdf\"\n",
    "instances_generated = []\n",
    "\n",
    "for idx, row in reports_df.head(10).iterrows():\n",
    "    ident = row['ident']\n",
    "    title = row['title']\n",
    "\n",
    "    pdf_path = os.path.join(reports_folder, f\"{ident}.pdf\")\n",
    "\n",
    "    if not os.path.exists(pdf_path):\n",
    "        instances_generated.append({\n",
    "            \"report_id\": ident,\n",
    "            \"status\": \"not-found\",\n",
    "            \"created_by\": model_name\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    text = extract_text(pdf_path)\n",
    "    if not text:\n",
    "        instances_generated.append({\n",
    "            \"report_id\": ident,\n",
    "            \"status\": \"no-text\",\n",
    "            \"created_by\": model_name\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    max_tokens_per_chunk = 3000\n",
    "    text_chunks = split_text_into_chunks(text, max_tokens_per_chunk)\n",
    "\n",
    "    prompts = load_prompts(ident, title)\n",
    "    instances = generate_instances(text_chunks, prompts)\n",
    "    \n",
    "    if instances and isinstance(instances, list):\n",
    "        for instance in instances:\n",
    "            if isinstance(instance, dict) and \"question\" in instance:\n",
    "                instance.update({\n",
    "                    \"report_id\": ident,\n",
    "                    \"status\": \"generated\",\n",
    "                    \"created_by\": model_name\n",
    "                })\n",
    "                instances_generated.append(instance)\n",
    "            else:\n",
    "                instances_generated.append({\n",
    "                    \"report_id\": ident,\n",
    "                    \"status\": \"error\",\n",
    "                    \"created_by\": model_name\n",
    "                })\n",
    "    else:\n",
    "        instances_generated.append({\n",
    "            \"report_id\": ident,\n",
    "            \"status\": \"error\",\n",
    "            \"created_by\": model_name\n",
    "        })\n",
    "    print(f\"Waiting 60 seconds to respect token rate limits...\")\n",
    "    time.sleep(60)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Save generated instances to JSON file\n",
    "with open(\"instances_generated.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(instances_generated, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buscar el archivo con mas texto para saber cual es el mas largo y tenerlo como referencia\n",
    "\n",
    "- el más grande tiene 162.000 palabras\n",
    "\n",
    "Buscando los 10 archivos PDF/CSV más grandes...\n",
    "\n",
    "Procesando los archivos...\n",
    "\n",
    "Resultados:\n",
    "+----+-------------------+--------+---------------+------------+\n",
    "|    | Archivo           | Tipo   |   Tamaño (MB) |   Palabras |\n",
    "+====+===================+========+===============+============+\n",
    "|  0 | 365-CI-01208.pdf  | PDF    |        415.01 |      27571 |\n",
    "+----+-------------------+--------+---------------+------------+\n",
    "|  1 | 119-CI-00045.pdf  | PDF    |        358.58 |     339421 |\n",
    "+----+-------------------+--------+---------------+------------+\n",
    "|  2 | 748-CI-00863.pdf  | PDF    |        327.56 |      91304 |\n",
    "+----+-------------------+--------+---------------+------------+\n",
    "|  3 | 748-CI-00864.pdf  | PDF    |        327.56 |      91304 |\n",
    "+----+-------------------+--------+---------------+------------+\n",
    "|  4 | 365-CI-01260.pdf  | PDF    |        270.28 |     219309 |\n",
    "+----+-------------------+--------+---------------+------------+\n",
    "|  5 | 119-CI-00315.pdf  | PDF    |        258.05 |          0 |\n",
    "+----+-------------------+--------+---------------+------------+\n",
    "|  6 | 1308-CI-02024.pdf | PDF    |        210.18 |     168609 |\n",
    "+----+-------------------+--------+---------------+------------+\n",
    "|  7 | 365-CI-01192.pdf  | PDF    |        169.5  |     154781 |\n",
    "+----+-------------------+--------+---------------+------------+\n",
    "|  8 | 365-CI-01242.pdf  | PDF    |        152.88 |     103932 |\n",
    "+----+-------------------+--------+---------------+------------+\n",
    "|  9 | 748-CI-00872.pdf  | PDF    |        142.92 |      87690 |\n",
    "+----+-------------------+--------+---------------+------------+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\usuario\\desktop\\univalle\\tesis\\tesis\\notebooks\\utils\\.conda\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: tabulate in c:\\users\\usuario\\desktop\\univalle\\tesis\\tesis\\notebooks\\utils\\.conda\\lib\\site-packages (0.9.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2 tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Buscando los 10 archivos PDF/CSV más grandes...\n",
      "\n",
      "Procesando los archivos...\n",
      "\n",
      "Resultados:\n",
      "+----+-------------------+--------+---------------+------------+\n",
      "|    | Archivo           | Tipo   |   Tamaño (MB) |   Palabras |\n",
      "+====+===================+========+===============+============+\n",
      "|  0 | 365-CI-01208.pdf  | PDF    |        415.01 |      27571 |\n",
      "+----+-------------------+--------+---------------+------------+\n",
      "|  1 | 119-CI-00045.pdf  | PDF    |        358.58 |     339421 |\n",
      "+----+-------------------+--------+---------------+------------+\n",
      "|  2 | 748-CI-00863.pdf  | PDF    |        327.56 |      91304 |\n",
      "+----+-------------------+--------+---------------+------------+\n",
      "|  3 | 748-CI-00864.pdf  | PDF    |        327.56 |      91304 |\n",
      "+----+-------------------+--------+---------------+------------+\n",
      "|  4 | 365-CI-01260.pdf  | PDF    |        270.28 |     219309 |\n",
      "+----+-------------------+--------+---------------+------------+\n",
      "|  5 | 119-CI-00315.pdf  | PDF    |        258.05 |          0 |\n",
      "+----+-------------------+--------+---------------+------------+\n",
      "|  6 | 1308-CI-02024.pdf | PDF    |        210.18 |     168609 |\n",
      "+----+-------------------+--------+---------------+------------+\n",
      "|  7 | 365-CI-01192.pdf  | PDF    |        169.5  |     154781 |\n",
      "+----+-------------------+--------+---------------+------------+\n",
      "|  8 | 365-CI-01242.pdf  | PDF    |        152.88 |     103932 |\n",
      "+----+-------------------+--------+---------------+------------+\n",
      "|  9 | 748-CI-00872.pdf  | PDF    |        142.92 |      87690 |\n",
      "+----+-------------------+--------+---------------+------------+\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from PyPDF2 import PdfReader\n",
    "from tabulate import tabulate\n",
    "\n",
    "def get_largest_files(directory, extensions=('.pdf', '.csv'), top_n=10):\n",
    "    \"\"\"Obtiene los archivos más grandes con extensiones específicas\"\"\"\n",
    "    files = []\n",
    "    for entry in os.scandir(directory):\n",
    "        if entry.is_file() and entry.name.lower().endswith(extensions):\n",
    "            files.append((entry.path, entry.stat().st_size))\n",
    "    \n",
    "    # Ordenar por tamaño (de mayor a menor) y tomar los top_n\n",
    "    files.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [file[0] for file in files[:top_n]]\n",
    "\n",
    "def count_words_pdf(file_path):\n",
    "    \"\"\"Cuenta palabras en un archivo PDF\"\"\"\n",
    "    try:\n",
    "        reader = PdfReader(file_path)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "        return len(text.split())\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando PDF {file_path}: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "def count_words_csv(file_path):\n",
    "    \"\"\"Cuenta palabras en un archivo CSV\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            return len(content.split())\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando CSV {file_path}: {str(e)}\")\n",
    "        return 0\n",
    "\n",
    "def process_files(file_paths):\n",
    "    \"\"\"Procesa los archivos y devuelve los resultados\"\"\"\n",
    "    results = []\n",
    "    for file_path in file_paths:\n",
    "        size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        if file_path.lower().endswith('.pdf'):\n",
    "            word_count = count_words_pdf(file_path)\n",
    "        else:\n",
    "            word_count = count_words_csv(file_path)\n",
    "        \n",
    "        results.append({\n",
    "            \"Archivo\": os.path.basename(file_path),\n",
    "            \"Tipo\": \"PDF\" if file_path.lower().endswith('.pdf') else \"CSV\",\n",
    "            \"Tamaño (MB)\": f\"{size_mb:.2f}\",\n",
    "            \"Palabras\": word_count\n",
    "        })\n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    directory = input(\"Ingrese la ruta del directorio a analizar (deje vacío para usar el actual): \").strip()\n",
    "    if not directory:\n",
    "        directory = os.getcwd()\n",
    "    \n",
    "    if not os.path.isdir(directory):\n",
    "        print(\"El directorio especificado no existe\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nBuscando los 10 archivos PDF/CSV más grandes...\")\n",
    "    largest_files = get_largest_files(directory)\n",
    "    \n",
    "    if not largest_files:\n",
    "        print(\"No se encontraron archivos PDF o CSV en el directorio\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nProcesando los archivos...\")\n",
    "    results = process_files(largest_files)\n",
    "    \n",
    "    # Mostrar resultados en tabla\n",
    "    print(\"\\nResultados:\")\n",
    "    print(tabulate(results, headers=\"keys\", tablefmt=\"grid\", showindex=True))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Instalar dependencias si no están disponibles\n",
    "    try:\n",
    "        from PyPDF2 import PdfReader\n",
    "        from tabulate import tabulate\n",
    "    except ImportError:\n",
    "        print(\"Instalando dependencias necesarias...\")\n",
    "        import subprocess\n",
    "        subprocess.run(['pip', 'install', 'PyPDF2', 'tabulate'], check=True)\n",
    "        from PyPDF2 import PdfReader\n",
    "        from tabulate import tabulate\n",
    "    \n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
